#!/usr/bin/env python3
"""
Kokoro vs MMS-TTS æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬ (ç”Ÿäº§ç¯å¢ƒè¯„ä¼°ç‰ˆ)
æµ‹é‡æŒ‡æ ‡: 
  - RTF (Real Time Factor)
  - TTFB (Time to First Byte/Audio)
"""
import os
import sys
import time
import gc
import re
import numpy as np
import scipy.io.wavfile as wav
from pathlib import Path
from loguru import logger

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

# æµ‹è¯•æ–‡æœ¬ (å¢åŠ åˆ° 4 æ®µ)
TEST_TEXTS = {
    "en": [
        "Warmup sentence to stabilize CUDA kernels.", # ç¬¬1æ¡å°†è¢«å‰”é™¤
        "Hello, this is a short sentence for testing.",
        "The quick brown fox jumps over the lazy dog. This is a medium length sentence to evaluate the quality of speech synthesis.",
        "Artificial intelligence is transforming the way we interact with technology. From voice assistants to autonomous vehicles, AI is becoming an integral part of our daily lives."
    ],
    "ms": [
        "Ayat pemanasan untuk menstabilkan kernel CUDA.", # ç¬¬1æ¡å°†è¢«å‰”é™¤
        "Halo, ini adalah ayat pendek untuk ujian.",
        "Saya adalah asisten AI yang dirancang untuk membantu Anda dengan berbagai tugas. Saya dapat menjawab pertanyaan dan membantu Anda.",
        "Kecerdasan buatan sedang mengubah cara kita berinteraksi dengan teknologi. Dari asisten suara hingga kendaraan otonom, AI menjadi bagian integral dari kehidupan sehari-hari kita."
    ]
}

def get_gpu_memory_mb():
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
    except: pass
    return -1

def get_peak_gpu_memory_mb():
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.max_memory_allocated() / 1024 / 1024
    except: pass
    return -1

def clear_gpu_memory():
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
    except: pass
    gc.collect()

def benchmark_kokoro(provider="auto"):
    try:
        from src.engines.kokoro_engine import KokoroEngine
        model_path = str(ROOT_DIR / "models" / "kokoro" / "kokoro-v1.0.onnx")
        voices_path = str(ROOT_DIR / "models" / "kokoro" / "voices-v1.0.bin")
        
        clear_gpu_memory()
        start_load = time.time()
        engine = KokoroEngine(model_path, voices_path)
        engine._load_model()
        load_time = time.time() - start_load
        
        # é¢„çƒ­è¿‡ç¨‹ (å†…éƒ¨é¢„çƒ­)
        engine.synthesize("Warmup.")
        
        results = {
            "model_name": "Kokoro-82M (ONNX + CUDA)",
            "load_time": load_time,
            "warmup_time": 0, # è¿™é‡Œç»Ÿä¸€ä¸º 0ï¼Œå› ä¸ºæˆ‘ä»¬å…³æ³¨ç¨³æ€
            "details": [],
            "output_files": []
        }
        
        output_dir = ROOT_DIR / "output" / "benchmark"
        output_dir.mkdir(parents=True, exist_ok=True)

        for i, text in enumerate(TEST_TEXTS["en"]):
            chunks = [c for c in re.split(r'([.!?])', text) if c.strip()]
            total_start = time.time()
            ttfb = 0
            all_audio = []
            
            for j, chunk in enumerate(chunks):
                if not chunk.strip(): continue
                chunk_audio = engine.synthesize(chunk)
                if j == 0:
                    ttfb = time.time() - total_start
                all_audio.append(chunk_audio)
            
            total_elapsed = time.time() - total_start
            combined_audio = np.concatenate(all_audio)
            duration = len(combined_audio) / 24000
            
            # åªè®°å½•ç´¢å¼• > 0 çš„æ•°æ® (å‰”é™¤ç¬¬ä¸€ä¸ªå†·å¯åŠ¨æ ·æœ¬)
            if i > 0:
                results["details"].append({
                    "char_len": len(text),
                    "elapsed": total_elapsed,
                    "ttfb": ttfb,
                    "duration": duration,
                    "rtf": total_elapsed / duration if duration > 0 else 0
                })
            
            out_file = output_dir / f"kokoro_steady_{i}.wav"
            wav.write(out_file, 24000, (combined_audio * 32767).astype(np.int16))
            results["output_files"].append(str(out_file))
            
        results["gpu_mem_peak"] = get_peak_gpu_memory_mb()
        results["gpu_mem_current"] = get_gpu_memory_mb()
        return results
    except Exception as e:
        logger.error(f"Kokoro Fail: {e}")
        return None

def benchmark_mms(device="cuda"):
    try:
        from src.engines.mms_engine import MMSEngine
        models_dir = str(ROOT_DIR / "models")
        
        clear_gpu_memory()
        start_load = time.time()
        engine = MMSEngine(models_dir, device=device)
        engine._load_model("ms")
        load_time = time.time() - start_load
        
        results = {
            "model_name": "Meta MMS-TTS (PyTorch + CUDA)",
            "load_time": load_time,
            "warmup_time": 0,
            "details": [],
            "output_files": []
        }
        
        output_dir = ROOT_DIR / "output" / "benchmark"
        
        for i, text in enumerate(TEST_TEXTS["ms"]):
            total_start = time.time()
            audio = engine.synthesize(text, language="ms")
            elapsed = time.time() - total_start
            
            duration = len(audio) / 16000
            
            if i > 0:
                results["details"].append({
                    "char_len": len(text),
                    "elapsed": elapsed,
                    "ttfb": elapsed,
                    "duration": duration,
                    "rtf": elapsed / duration if duration > 0 else 0
                })
            
            out_file = output_dir / f"mms_steady_{i}.wav"
            wav.write(out_file, 16000, (audio * 32767).astype(np.int16))
            results["output_files"].append(str(out_file))
            
        results["gpu_mem_peak"] = get_peak_gpu_memory_mb()
        results["gpu_mem_current"] = get_gpu_memory_mb()
        return results
    except Exception as e:
        logger.error(f"MMS Fail: {e}")
        return None

def print_comparison(results_list):
    print("\n" + "=" * 95)
    print("ğŸš€ Kokoro TTS ç”Ÿäº§æ€§èƒ½è¯„ä¼°æŠ¥å‘Š (ç¨³æ€æ•°æ®)")
    print("=" * 95)
    print("\n[æŒ‡æ ‡å®šä¹‰è¯´æ˜]:")
    print(" - TTFB (Time To First Byte): é¦–éŸ³å»¶è¿Ÿã€‚æŒ‡ä»è¯·æ±‚å¼€å§‹åˆ°ç”Ÿæˆç¬¬ä¸€å¥éŸ³é¢‘çš„æ—¶é—´ã€‚æ­¤æ•°å€¼è¶Šå°ï¼Œç”¨æˆ·çš„ä¸»è§‚â€œç§’å¼€â€æ„ŸçŸ¥è¶Šå¼ºã€‚")
    print(" - RTF (Real Time Factor): å®æ—¶ç‡ã€‚è®¡ç®—å…¬å¼ä¸º [æ¨ç†æ—¶é•¿ / éŸ³é¢‘æ—¶é•¿]ã€‚")
    print("   * RTF < 1: æ¨ç†é€Ÿåº¦å¿«äºè¯­é€Ÿï¼Œä¸ä¼šå‡ºç°å¡é¡¿ã€‚")
    print("   * RTF < 0.1: é¡¶çº§æ€§èƒ½ï¼Œä»£è¡¨ 10 ç§’è¯­éŸ³ä»…éœ€ 1 ç§’åˆæˆã€‚")
    print(" - Total (s): ç”Ÿæˆæ•´æ®µå®Œæ•´è¯è¯­æ‰€éœ€çš„æ€»æ—¶é—´ã€‚")

    print("\nâ±ï¸  ç¨³æ€æ€§èƒ½æ•°æ® (å·²å‰”é™¤åˆæ¬¡å†·å¯åŠ¨å¹²æ‰°):")
    header = f"   {'æ¨¡å‹':<30} {'å­—æ•°':<6} {'Total(s)':<10} {'TTFB(s)':<10} {'éŸ³é¢‘æ—¶é•¿(s)':<12} {'RTF':<8}"
    print(header)
    print("   " + "-" * 90)
    for r in results_list:
        for item in r['details']:
            print(f"   {r['model_name']:<30} {item['char_len']:<6} {item['elapsed']:<10.2f} {item['ttfb']:<10.2f} {item['duration']:<12.2f} x {item['rtf']:.3f}")
    
    print("\nğŸ’¾ GPU èµ„æºå ç”¨ (Tesla V100):")
    for r in results_list:
        print(f"   {r['model_name']:<30} æ˜¾å­˜å ç”¨: {r['gpu_mem_current']:.1f} MB | å³°å€¼: {r['gpu_mem_peak']:.1f} MB")
    print("=" * 95 + "\n")

def main():
    results = []
    res_k = benchmark_kokoro("gpu")
    if res_k: results.append(res_k)
    res_m = benchmark_mms("cuda")
    if res_m: results.append(res_m)
    print_comparison(results)

if __name__ == "__main__":
    main()
