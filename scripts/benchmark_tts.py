#!/usr/bin/env python3
"""
Kokoro vs MMS-TTS æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬

æµ‹è¯•ç»´åº¦:
  â€¢ æ¨¡å‹åŠ è½½æ—¶é—´
  â€¢ æ¨ç†é€Ÿåº¦ (CPU vs GPU)
  â€¢ GPU æ˜¾å­˜å ç”¨
  â€¢ éŸ³è´¨å¯¹æ¯” (æ‰‹åŠ¨)

æ”¯æŒ:
  â€¢ Kokoro-82M (ONNX, CPU/GPU)
  â€¢ Meta MMS-TTS (PyTorch, CPU/GPU)
"""
import os
import sys
import time
import gc
from pathlib import Path

ROOT_DIR = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(ROOT_DIR))

from loguru import logger

# æµ‹è¯•æ–‡æœ¬
TEST_TEXTS = {
    "en": [
        "Hello, this is a short sentence for testing.",
        "The quick brown fox jumps over the lazy dog. This is a medium length sentence to evaluate the quality of speech synthesis.",
        "Artificial intelligence is transforming the way we interact with technology. From voice assistants to autonomous vehicles, AI is becoming an integral part of our daily lives.",
    ],
    "ms": [
        "Halo, ini adalah ayat pendek untuk ujian.",  # çŸ­å¥
        "Saya adalah asisten AI yang dirancang untuk membantu Anda dengan berbagai tugas. Saya dapat menjawab pertanyaan, memberikan informasi, dan membantu Anda menyelesaikan pekerjaan.",  # ä¸­å¥
        "Kecerdasan buatan sedang mengubah cara kita berinteraksi dengan teknologi. Dari asisten suara hingga kendaraan otonom, AI menjadi bagian integral dari kehidupan sehari-hari kita. Teknologi ini terus berkembang dan memberikan manfaat luar biasa bagi masyarakat.",  # é•¿å¥
    ]
}

def get_gpu_memory_mb():
    """è·å–å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨é‡ (MB)"""
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
    except:
        pass
    return -1

def get_peak_gpu_memory_mb():
    """è·å–å³°å€¼ GPU æ˜¾å­˜ (MB)"""
    try:
        import torch
        if torch.cuda.is_available():
            return torch.cuda.max_memory_allocated() / 1024 / 1024
    except:
        pass
    return -1

def clear_gpu_memory():
    """æ¸…ç† GPU ç¼“å­˜"""
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
        gc.collect()
    except:
        pass

def benchmark_kokoro(provider="auto"):
    """
    æµ‹è¯• Kokoro-82M
    
    Args:
        provider: ONNX æ‰§è¡Œæä¾›è€… ("auto", "cpu", "gpu")
    """
    try:
        from src.engines.kokoro_engine import KokoroEngine
    except ImportError:
        logger.error("âŒ KokoroEngine æœªæ‰¾åˆ°")
        return None
    
    model_path = str(ROOT_DIR / "models" / "kokoro" / "kokoro-v1.0.onnx")
    voices_path = str(ROOT_DIR / "models" / "kokoro" / "voices.json")
    
    if not os.path.exists(model_path) or not os.path.exists(voices_path):
        logger.error("âŒ Kokoro æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
        return None
    
    # è®¾ç½® ONNX provider
    if provider == "cpu":
        os.environ["ONNX_PROVIDER"] = "CPUExecutionProvider"
        model_name = "Kokoro-82M (CPU)"
    elif provider == "gpu":
        os.environ["ONNX_PROVIDER"] = "CUDAExecutionProvider"
        model_name = "Kokoro-82M (GPU)"
    else:
        os.environ.pop("ONNX_PROVIDER", None)
        model_name = "Kokoro-82M (Auto)"
    
    results = {
        "model": model_name,
        "load_time": 0,
        "warmup_time": 0,
        "synthesis_times": [],
        "gpu_memory_mb": -1,
        "peak_gpu_memory_mb": -1,
    }
    
    try:
        clear_gpu_memory()
        mem_before = get_gpu_memory_mb()
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"ğŸ“¥ [Kokoro] åŠ è½½æ¨¡å‹ ({provider} mode)...")
        start = time.time()
        engine = KokoroEngine(model_path, voices_path)
        engine._load_model()
        results["load_time"] = time.time() - start
        logger.info(f"âœ… [Kokoro] æ¨¡å‹åŠ è½½: {results['load_time']:.2f}s")
        
        mem_after = get_gpu_memory_mb()
        if mem_before >= 0 and mem_after >= 0:
            results["gpu_memory_mb"] = mem_after - mem_before
        
        # é¢„çƒ­
        logger.info("ğŸ”¥ [Kokoro] é¢„çƒ­...")
        start = time.time()
        engine.synthesize("Warmup test.", voice="af_sarah", lang="en-us")
        results["warmup_time"] = time.time() - start
        logger.info(f"âœ… [Kokoro] é¢„çƒ­: {results['warmup_time']:.2f}s")
        
        # åˆæˆæµ‹è¯•
        output_dir = ROOT_DIR / "output" / "benchmark"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("â±ï¸  [Kokoro] å¼€å§‹åˆæˆæµ‹è¯•...")
        for i, text in enumerate(TEST_TEXTS["en"]):
            output_file = str(output_dir / f"kokoro_{provider}_{i+1}.wav")
            audio = engine.synthesize(text, voice="af_sarah", lang="en-us", output_path=output_file)
            elapsed = time.time() - start
            
            # è®¡ç®—éŸ³é¢‘æ—¶é•¿
            duration = len(audio) / 24000  # Kokoro é‡‡æ ·ç‡å›ºå®š 24k
            
            results["synthesis_times"].append({
                "text_length": len(text),
                "time_seconds": elapsed,
                "duration": duration,
                "output_file": output_file,
            })

            logger.info(f"  âœ“ Text {i+1} ({len(text)} chars): {elapsed:.2f}s")
        
        results["peak_gpu_memory_mb"] = get_peak_gpu_memory_mb()
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ Kokoro æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def benchmark_mms(device="auto"):
    """
    æµ‹è¯• Meta MMS-TTS
    
    Args:
        device: è®¡ç®—è®¾å¤‡ ("auto", "cpu", "cuda")
    """
    try:
        from src.engines.mms_engine import MMSEngine
    except ImportError:
        logger.error("âŒ MMSEngine æœªæ‰¾åˆ°")
        return None
    
    models_dir = str(ROOT_DIR / "models")
    
    if device == "auto":
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = f"MMS-TTS (Auto -> {device.upper()})"
    elif device == "cpu":
        model_name = "MMS-TTS (CPU)"
    elif device == "cuda":
        model_name = "MMS-TTS (GPU)"
    else:
        device = "cpu"
        model_name = "MMS-TTS (CPU)"
    
    results = {
        "model": model_name,
        "device": device,
        "load_time": 0,
        "warmup_time": 0,
        "synthesis_times": [],
        "gpu_memory_mb": -1,
        "peak_gpu_memory_mb": -1,
    }
    
    try:
        clear_gpu_memory()
        mem_before = get_gpu_memory_mb()
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"ğŸ“¥ [MMS] åŠ è½½æ¨¡å‹ (device={device})...")
        start = time.time()
        engine = MMSEngine(models_dir, device=device)
        engine._load_model("ms")  # é¢„åŠ è½½é©¬æ¥æ–‡æ¨¡å‹
        results["load_time"] = time.time() - start
        logger.info(f"âœ… [MMS] æ¨¡å‹åŠ è½½: {results['load_time']:.2f}s")
        
        mem_after = get_gpu_memory_mb()
        if mem_before >= 0 and mem_after >= 0:
            results["gpu_memory_mb"] = mem_after - mem_before
        
        # é¢„çƒ­
        logger.info("ğŸ”¥ [MMS] é¢„çƒ­...")
        start = time.time()
        engine.synthesize("Ujian", language="ms")
        results["warmup_time"] = time.time() - start
        logger.info(f"âœ… [MMS] é¢„çƒ­: {results['warmup_time']:.2f}s")
        
        # åˆæˆæµ‹è¯• (é©¬æ¥æ–‡)
        output_dir = ROOT_DIR / "output" / "benchmark"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("â±ï¸  [MMS] å¼€å§‹åˆæˆæµ‹è¯• (Malay/é©¬æ¥æ–‡)...")
        for i, text in enumerate(TEST_TEXTS["ms"]):
            output_file = str(output_dir / f"mms_{device}_{i+1}.wav")
            audio = engine.synthesize(text, language="ms", output_path=output_file)
            elapsed = time.time() - start
            
            # è®¡ç®—éŸ³é¢‘æ—¶é•¿
            sample_rate = engine.get_sample_rate("ms")
            duration = len(audio) / sample_rate
            
            results["synthesis_times"].append({
                "text_length": len(text),
                "time_seconds": elapsed,
                "duration": duration,
                "output_file": output_file,
            })

            logger.info(f"  âœ“ Text {i+1} ({len(text)} chars): {elapsed:.2f}s")
        
        results["peak_gpu_memory_mb"] = get_peak_gpu_memory_mb()
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ MMS æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def print_comparison(results_list):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š TTS æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœ")
    print("=" * 80)
    
    if not results_list:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    # 1. æ¨¡å‹åŠ è½½æ—¶é—´
    print("\nğŸ”„ æ¨¡å‹åŠ è½½æ—¶é—´ (ä¸€æ¬¡æ€§å¼€é”€):")
    for res in results_list:
        print(f"   {res['model']:<30} {res['load_time']:.2f}s")
    
    # 2. é¢„çƒ­æ—¶é—´
    print("\nğŸ”¥ æ¨¡å‹é¢„çƒ­æ—¶é—´ (é¦–æ¬¡æ¨ç†):")
    for res in results_list:
        print(f"   {res['model']:<30} {res.get('warmup_time', 0):.2f}s")
    
    # 3. GPU æ˜¾å­˜
    print("\nğŸ’¾ GPU æ˜¾å­˜å ç”¨:")
    for res in results_list:
        if res['gpu_memory_mb'] >= 0:
            print(f"   {res['model']:<30} {res['gpu_memory_mb']:.1f} MB (å½“å‰)")
        else:
            print(f"   {res['model']:<30} N/A (CPU æ¨¡å¼)")
        
        if res['peak_gpu_memory_mb'] >= 0:
            print(f"   {'   (å³°å€¼)':<30} {res['peak_gpu_memory_mb']:.1f} MB")
    
    # 4. åˆæˆé€Ÿåº¦å¯¹æ¯” (è¯¦ç»†æŠ¥è¡¨)
    print("\nâ±ï¸  åˆæˆé€Ÿåº¦å¯¹æ¯” (è¯¦ç»†æŠ¥è¡¨):")
    header = f"   {'æ¨¡å‹':<25} {'æ–‡æœ¬':<6} {'è€—æ—¶(s)':<8} {'æ—¶é•¿(s)':<8} {'é€Ÿåº¦':<8} {'RTF':<8}"
    print(header)
    print("   " + "-" * len(header))
    
    for res in results_list:
        for item in res['synthesis_times']:
            text_len = item['text_length']
            time_sec = item['time_seconds']
            duration = item['duration']
            
            speed = duration / time_sec if time_sec > 0 else 0
            rtf = time_sec / duration if duration > 0 else 0
            
            print(f"   {res['model']:<25} {text_len:<6} {time_sec:<8.2f} {duration:<8.2f} {speed:<8.1f}x {rtf:<8.3f}")

    
    # 5. éŸ³é¢‘æ–‡ä»¶ä½ç½®
    print("\nğŸµ ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶:")
    output_dir = ROOT_DIR / "output" / "benchmark"
    print(f"   ä¿å­˜ä½ç½®: {output_dir}")
    print(f"   æ–‡ä»¶: ")
    if output_dir.exists():
        for wav_file in sorted(output_dir.glob("*.wav")):
            print(f"      â€¢ {wav_file.name}")
    
    print("\nğŸ“ æµ‹è¯•å®Œæˆ! è¯·æ‰‹åŠ¨å¯¹æ¯”éŸ³è´¨å·®å¼‚")
    print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    logger.remove()
    logger.add(sys.stderr, format="<level>{message}</level>", level="INFO")
    
    import argparse
    parser = argparse.ArgumentParser(
        description="TTS æ€§èƒ½å¯¹æ¯”æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é»˜è®¤: Kokoro Auto + MMS Auto
  python scripts/benchmark_tts.py
  
  # GPU æ¨¡å¼å¯¹æ¯”
  python scripts/benchmark_tts.py --kokoro gpu --mms gpu
  
  # ä»…æµ‹è¯• Kokoro
  python scripts/benchmark_tts.py --kokoro both --skip-mms
  
  # CPU vs GPU å¯¹æ¯”
  python scripts/benchmark_tts.py --kokoro both --mms gpu
        """
    )
    
    parser.add_argument(
        "--kokoro",
        choices=["auto", "cpu", "gpu", "both"],
        default="auto",
        help="Kokoro æµ‹è¯•æ¨¡å¼"
    )
    parser.add_argument(
        "--mms",
        choices=["auto", "cpu", "gpu"],
        default="auto",
        help="MMS æµ‹è¯•æ¨¡å¼"
    )
    parser.add_argument(
        "--skip-kokoro",
        action="store_true",
        help="è·³è¿‡ Kokoro æµ‹è¯•"
    )
    parser.add_argument(
        "--skip-mms",
        action="store_true",
        help="è·³è¿‡ MMS æµ‹è¯•"
    )
    
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info("ğŸš€ TTS æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    logger.info("=" * 80)
    
    results_list = []
    
    # æµ‹è¯• Kokoro
    if not args.skip_kokoro:
        if args.kokoro == "both":
            logger.info("\n--- Kokoro-82M (CPU) ---")
            clear_gpu_memory()
            result = benchmark_kokoro(provider="cpu")
            if result:
                results_list.append(result)
            
            logger.info("\n--- Kokoro-82M (GPU) ---")
            clear_gpu_memory()
            result = benchmark_kokoro(provider="gpu")
            if result:
                results_list.append(result)
        else:
            logger.info(f"\n--- Kokoro-82M ({args.kokoro}) ---")
            clear_gpu_memory()
            result = benchmark_kokoro(provider=args.kokoro)
            if result:
                results_list.append(result)
    
    # æµ‹è¯• MMS
    if not args.skip_mms:
        logger.info(f"\n--- Meta MMS-TTS ({args.mms}) ---")
        clear_gpu_memory()
        result = benchmark_mms(device=args.mms)
        if result:
            results_list.append(result)
    
    # æ‰“å°ç»“æœ
    print_comparison(results_list)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
