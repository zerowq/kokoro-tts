"""
Meta MMS-TTS å¼•æ“å°è£…

æ”¯æŒé©¬æ¥æ–‡ã€å°åº¦å°¼è¥¿äºšæ–‡ç­‰å¤šè¯­è¨€
é€‚åˆå¤„ç† Kokoro ä¸æ”¯æŒçš„è¯­è¨€
"""
import os
import torch
import numpy as np
import scipy.io.wavfile as wav
from typing import Optional, Dict
from transformers import VitsModel, AutoTokenizer
from pathlib import Path
from loguru import logger

class MMSEngine:
    """Meta MMS-TTS å¤šè¯­è¨€å¼•æ“"""
    
    # æ”¯æŒçš„è¯­è¨€æ¨¡å‹æ˜ å°„
    LANGUAGE_MODELS = {
        "en": "mms-tts-eng",      # English
        "ms": "mms-tts-zlm",      # Malay (é©¬æ¥æ–‡)
        "id": "mms-tts-ind",      # Indonesian
        "zh": "mms-tts-zho",      # Chinese
        "ja": "mms-tts-jpn",      # Japanese
        "ko": "mms-tts-kor",      # Korean
        "es": "mms-tts-spa",      # Spanish
        "fr": "mms-tts-fra",      # French
        "de": "mms-tts-deu",      # German
        "it": "mms-tts-ita",      # Italian
    }
    
    def __init__(self, model_dir: str, device: Optional[str] = None):
        """
        åˆå§‹åŒ– MMS å¼•æ“
        
        Args:
            model_dir: æ¨¡å‹å­˜å‚¨ç›®å½•
            device: è®¡ç®—è®¾å¤‡ ('cpu', 'cuda', æˆ– None è‡ªåŠ¨é€‰æ‹©)
        """
        self.model_dir = Path(model_dir)
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self._models: Dict[str, VitsModel] = {}
        self._tokenizers: Dict[str, AutoTokenizer] = {}
        
        logger.info(f"ğŸ¤ MMS Engine initialized on {self.device.upper()}")
    
    def _load_model(self, language: str):
        """
        åŠ è½½æŒ‡å®šè¯­è¨€çš„æ¨¡å‹
        
        Args:
            language: è¯­è¨€ä»£ç  (å¦‚ 'ms' é©¬æ¥æ–‡, 'en' è‹±æ–‡)
        """
        if language in self._models:
            return  # å·²åŠ è½½ï¼Œç›´æ¥è¿”å›
        
        model_name = self.LANGUAGE_MODELS.get(language)
        if not model_name:
            raise ValueError(f"âŒ Unsupported language: {language}. Supported: {list(self.LANGUAGE_MODELS.keys())}")
        
        try:
            # ä¼˜å…ˆä»æœ¬åœ° model_dir åŠ è½½
            local_model_path = self.model_dir / model_name
            
            if local_model_path.exists():
                logger.info(f"ğŸ“¥ Loading local MMS-TTS from {local_model_path}...")
                self._models[language] = VitsModel.from_pretrained(
                    local_model_path, 
                    local_files_only=True
                ).to(self.device)
                self._tokenizers[language] = AutoTokenizer.from_pretrained(
                    local_model_path,
                    local_files_only=True
                )
            else:
                logger.warning(f"âš ï¸ Local model not found at {local_model_path}")
                logger.info(f"ğŸ“¥ Downloading MMS-TTS from Hugging Face (facebook/{model_name})...")
                self._models[language] = VitsModel.from_pretrained(
                    f"facebook/{model_name}"
                ).to(self.device)
                self._tokenizers[language] = AutoTokenizer.from_pretrained(
                    f"facebook/{model_name}"
                )
            
            logger.info(f"âœ… MMS-TTS ({language}) loaded successfully on {self.device.upper()}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load MMS model for language {language}: {e}")
            raise
    
    def get_sample_rate(self, language: str = "ms") -> int:
        """è·å–æŒ‡å®šè¯­è¨€çš„é‡‡æ ·ç‡"""
        self._load_model(language)
        return self._models[language].config.sampling_rate
    
    def synthesize(
        self, 
        text: str, 
        language: str = "ms",
        output_path: Optional[str] = None
    ) -> np.ndarray:
        """
        åˆæˆè¯­éŸ³
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            language: è¯­è¨€ä»£ç  (é»˜è®¤ 'ms' é©¬æ¥æ–‡)
            output_path: å¯é€‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ³¢å½¢æ•°æ® (numpy array)
        """
        self._load_model(language)
        model = self._models[language]
        tokenizer = self._tokenizers[language]
        
        # æ–‡æœ¬è½¬ token
        inputs = tokenizer(text, return_tensors="pt").to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            output = model(**inputs).waveform
        
        # æå–å¹¶è½¬æ¢æ³¢å½¢
        waveform = output.squeeze().cpu().numpy()
        
        # ä¿å­˜åˆ°æ–‡ä»¶ (å¦‚æœæŒ‡å®š)
        if output_path:
            os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
            sample_rate = model.config.sampling_rate
            wav.write(output_path, rate=sample_rate, data=waveform)
            logger.info(f"âœ… Audio saved to {output_path}")
        
        return waveform
    
    def get_supported_languages(self) -> Dict[str, str]:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return self.LANGUAGE_MODELS.copy()
    
    def clear_cache(self):
        """æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜"""
        self._models.clear()
        self._tokenizers.clear()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        logger.info("âœ… Model cache cleared")
