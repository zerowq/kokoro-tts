"""
Kokoro GPU è¯Šæ–­è„šæœ¬
"""
import os
import sys
from pathlib import Path

ROOT_DIR = Path(__file__).parent.absolute()

print("=" * 70)
print("ğŸ” Kokoro GPU è¯Šæ–­")
print("=" * 70)

# 1. æ£€æŸ¥ onnxruntime-gpu æ˜¯å¦å®‰è£…
print("\n1ï¸âƒ£ æ£€æŸ¥ onnxruntime-gpu å®‰è£…:")
import importlib.util
spec = importlib.util.find_spec("onnxruntime-gpu")
print(f"   onnxruntime-gpu spec: {spec}")

# 2. æ£€æŸ¥ onnxruntime ç‰ˆæœ¬å’Œå¯ç”¨ providers
print("\n2ï¸âƒ£ æ£€æŸ¥ ONNX Runtime:")
import onnxruntime as ort
print(f"   Version: {ort.__version__}")
print(f"   Available providers: {ort.get_available_providers()}")

# 3. æµ‹è¯•ä¸åŒçš„ provider è®¾ç½®æ–¹å¼
print("\n3ï¸âƒ£ æµ‹è¯• Kokoro åˆå§‹åŒ–:")

model_path = str(ROOT_DIR / "models" / "kokoro" / "kokoro-v1.0.onnx")
voices_path = str(ROOT_DIR / "models" / "kokoro" / "voices.json")

# æ–¹å¼1: é€šè¿‡ç¯å¢ƒå˜é‡
print("\n   æ–¹å¼1: è®¾ç½® ONNX_PROVIDER ç¯å¢ƒå˜é‡")
os.environ["ONNX_PROVIDER"] = "CUDAExecutionProvider"
from kokoro_onnx import Kokoro
k1 = Kokoro(model_path, voices_path)
print(f"   Session providers: {k1.sess.get_providers()}")

# æ–¹å¼2: ç›´æ¥åˆ›å»º InferenceSession
print("\n   æ–¹å¼2: ç›´æ¥æŒ‡å®š providers")
sess = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
print(f"   Session providers: {sess.get_providers()}")

# 4. æµ‹è¯•å®é™…æ¨ç†æ—¶çš„ GPU ä½¿ç”¨
print("\n4ï¸âƒ£ æµ‹è¯•æ¨ç†æ—¶ GPU ä½¿ç”¨:")
import torch
if torch.cuda.is_available():
    print(f"   æ¨ç†å‰ GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024 / 1024:.1f} MB")

    # ä½¿ç”¨æ–¹å¼1çš„ Kokoro å®ä¾‹
    audio = k1.create("Hello world", voice="af_sarah", lang="en-us")

    print(f"   æ¨ç†å GPU æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024 / 1024:.1f} MB")
    print(f"   ç”ŸæˆéŸ³é¢‘é•¿åº¦: {len(audio)} samples")
else:
    print("   âš ï¸ CUDA ä¸å¯ç”¨")

print("\n" + "=" * 70)
